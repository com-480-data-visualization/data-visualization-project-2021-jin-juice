{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "retained-sewing",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-difficulty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import ujson as json\n",
    "import pandas as pd\n",
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import warnings\n",
    "import ast\n",
    "import glob\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-travel",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-keyboard",
   "metadata": {},
   "source": [
    "## Import all parler data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "parler_files = glob.glob('./parler_data_csv/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "parler_df = parler_df.dropna(subset=['body', 'bodywithurls']) # Removes empty messages\n",
    "parler_df = parler_df[parler_df.createdAtformatted > '2020-01-01'] # Keep messages from 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "parler_df['createdAtformatted'] = pd.to_datetime(parler_df['createdAtformatted']) # Convert to date object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "parler_df['date'] = parler_df['createdAtformatted']\n",
    "parler_df['date'] =  parler_df['date'].dt.date\n",
    "\n",
    "parler_hashtags_df = parler_df[parler_df.body.str.contains('#')].copy() # Select columns containing hashtages\n",
    "# convert hashtag column to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "parler_df_hashtags = parler_hashtags_df[['date','body']]\n",
    "parler_df_hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-auditor",
   "metadata": {},
   "source": [
    "## Extract hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_df_hashtag_count(parler_df):\n",
    "    parler_df = parler_df.dropna(subset=['body', 'bodywithurls'])\n",
    "    parler_df = parler_df[parler_df.createdAtformatted > '2020-01-01']\n",
    "    parler_df['createdAtformatted'] = pd.to_datetime(parler_df['createdAtformatted'])\n",
    "    parler_df['date'] = parler_df['createdAtformatted']\n",
    "    parler_df['date'] =  parler_df['date'].dt.date\n",
    "    parler_hashtags_df = parler_df[parler_df.body.str.contains('#')].copy()\n",
    "    parler_hashtags_df['hashtags'] = parler_hashtags_df.hashtags.str.lower().apply(lambda s: list(ast.literal_eval(s)))\n",
    "    parler_df_hashtags = parler_hashtags_df[['date','body']]\n",
    "    agg = parler_df_hashtags.groupby('date').apply(lambda x: x.body.str.extractall(r'(\\#\\w+)')[0].value_counts()).to_frame()\n",
    "    agg = agg.reset_index().rename(columns = {'level_1': 'hashtag',0:'value'})\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-angel",
   "metadata": {},
   "source": [
    "For each CSV file we group by date and hashtag, by counting the number of time the hashtag is used. We then export them as `agg_{i}.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob('./parler_data_csv/*.csv')\n",
    "for i, file in enumerate(glob.glob('./parler_data_csv/*.csv')):\n",
    "    print(f'aggregating_data for file {file} ({i+1}/{len(file_list)})')\n",
    "    df = pd.read_csv(file)\n",
    "    agg = aggregate_df_hashtag_count(df)\n",
    "    agg.to_csv(f'agg_{i}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-external",
   "metadata": {},
   "source": [
    "# Aggregating each file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in glob.glob('agg*.csv'):\n",
    "    df = pd.read_csv(file)\n",
    "    print(df.shape)\n",
    "    df = df.sort_values('value',ascending=False)\n",
    "#     We drop hashtags that are automatic when joining Parler\n",
    "    df = df[df['hashtag'] != '#parlerconcierge']\n",
    "    df = df[df['hashtag'] != '#Parler']\n",
    "    df = df[df['hashtag'] != '#parler']\n",
    "    df = df[df['hashtag'] != '#parlerConcierge']\n",
    "    df = df[df['hashtag'] != '#ParlerConcierge']\n",
    "    df = df[df['hashtag'] != '#newuser']\n",
    "    df = df.drop('Unnamed: 0', axis=1)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = pd.concat(dfs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-configuration",
   "metadata": {},
   "source": [
    "# Final aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df['hashtag'] = agg_df['hashtag'].apply(lambda x:str(x).lower()) # We lower each hashtag before the final aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-advisory",
   "metadata": {},
   "source": [
    "## Grouping by day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = agg_df.groupby(['date', 'hashtag']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.sort_values('value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_hashtags = agg_df.groupby(['hashtag']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-croatia",
   "metadata": {},
   "source": [
    "## Remove hashtags that are not used much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_hashtags = low_hashtags[low_hashtags['value']<5000].sort_values('value', ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = low_hashtags.hashtag.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_export = agg_df[~agg_df['hashtag'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_export['date'] = pd.to_datetime(agg_export['date']) # Convert to datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-lawsuit",
   "metadata": {},
   "source": [
    "## Aggregate hashtags per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = agg_export.groupby([pd.Grouper(key='date',freq='M'), 'hashtag']).sum()\n",
    "df1 = df1.reset_index()\n",
    "df1 = df1.rename({'hashtag': 'name'}, axis=1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-planet",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "res = json.loads(df1.to_json(orient='records'))\n",
    "\n",
    "\n",
    "with open('top_hashtags_month.json', 'w+') as outfile:\n",
    "    json.dump(res, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
